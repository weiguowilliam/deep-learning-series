{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import util\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "beta1 = 0.5\n",
    "batch_size = 64\n",
    "epi = 2\n",
    "save_sample_period = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('samples'):\n",
    "    os.mkdir('samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrelu(x,alpha = 0.2):\n",
    "    return max(x*alpha,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer:\n",
    "    def __init__(self, name, mi, mo, apply_batch_norm, filter_size = 5, stride = 2, f = tf.nn.relu):\n",
    "        self.W = tf.get_variable(\n",
    "            name = \"W_%s\"%name,\n",
    "            shape = (filter_size, filter_size, mi, mo),\n",
    "            initializer = tf.truncated_normal_initializer(stddev=0.02),\n",
    "        )\n",
    "        \n",
    "        self.b = tf.get_variable(\n",
    "            name = \"b_%s\"%name,\n",
    "            shape=(mo,),\n",
    "            initializer = tf.zeros_initializer(),\n",
    "        )\n",
    "        self.name = name,\n",
    "        self.f = f,\n",
    "        self.stride = stride,\n",
    "        self.apply_batch_norm = apply_batch_norm,\n",
    "        self.params = [self.W, self.b]\n",
    "        \n",
    "    def forward(self, X, reuse, is_training):\n",
    "        conv_out = tf.nn.conv2d(\n",
    "            X,\n",
    "            self.W,\n",
    "            strides = [1,self.stride,self.stride, 1],\n",
    "            padding = 'SAME'\n",
    "        )\n",
    "        conv_out = tf.nn.bias_add(conv_out,self.b)\n",
    "        \n",
    "        if self.apply_batch_norm:\n",
    "            conv_out = tf.contrib.layers.batch_norm(\n",
    "                conv_out,\n",
    "                decay = 0.9,\n",
    "                updates_collection = None,\n",
    "                epsilon = 1e-5,\n",
    "                scale = True,\n",
    "                is_training = is_training,\n",
    "                reuse = reuse,\n",
    "                scope = self.name,\n",
    "            )\n",
    "        \n",
    "        return self.f(conv_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FractioanllyStridedConvLayer:\n",
    "    def __init__(self, name, mi, mo, output_shape, apply_batch_norm, filter_size = 5, stride = 2, f = tf.nn.relu):\n",
    "        #here the order of mi and mo is reversed\n",
    "        self.W = tf.get_variable(\n",
    "            name = \"W_%s\"%name,\n",
    "            shape = (filter_size, filter_size,mo,mi),\n",
    "            initializer = tf.random_normal_initializer(stddev=0.02),\n",
    "        )\n",
    "        self.b = tf.get_variable(\n",
    "            name = \"b_%s\"%name,\n",
    "            shape = (mo,),\n",
    "            initializer = tf.zeros_initializer(),\n",
    "        )\n",
    "        self.f = f,\n",
    "        self.name = name,\n",
    "        self.output_shape = output_shape,\n",
    "        self.stride = stride,\n",
    "        self.apply_batch_norm = apply_batch_norm,\n",
    "        self.params = [self.W, self.b]\n",
    "        \n",
    "    def forward(self, X, reuse, is_training):\n",
    "        conv_out = tf.nn.conv2d_transpose(\n",
    "            value = X,\n",
    "            filter = self.W,\n",
    "            output_shape = self.output_shape,\n",
    "            strides = [1,self.stride, self.stride, 1],            \n",
    "        )\n",
    "        \n",
    "        conv_out = tf.nn.bias_add(conv_out,self.b)\n",
    "        \n",
    "        if self.apply_batch_norm:\n",
    "            conv_out = tf.contrib.layers.batch_norm(\n",
    "                conv_out,\n",
    "                decay = 0.9,\n",
    "                updates_collection = None,\n",
    "                epsilon = 1e-5,\n",
    "                scale = True,\n",
    "                is_training = is_training,\n",
    "                reuse = reuse,\n",
    "                scope = self.name,\n",
    "            )\n",
    "        \n",
    "        return self.f(conv_out)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "    def __init__(self, name, M1, M2, apply_batch_norm, f= tf.nn.relu):\n",
    "        self.W = tf.get_variable(\n",
    "            \"W_%s\"%name,\n",
    "            shape = (M1, M2),\n",
    "            initializer = tf.random_normal_initializer(stddev=0.02),\n",
    "        )\n",
    "        \n",
    "        self.b = tf.get_variable(\n",
    "            \"b_%s\"%name,\n",
    "            shape = (M2,),\n",
    "            initializer = tf.zeros_initializer(),\n",
    "        )\n",
    "        \n",
    "        self.name = name,\n",
    "        self.apply_batch_norm = apply_batch_norm,\n",
    "        self.f = f,\n",
    "        self.params = [self.W, self.b]\n",
    "    \n",
    "    def forward(self, X, reuse, is_training):\n",
    "        a = tf.matmul(X, self.W) + self.b\n",
    "        \n",
    "        if self.apply_batch_norm:\n",
    "            a = tf.contrib.layers.batch_norm(\n",
    "                a,\n",
    "                decay = 0.9,\n",
    "                updates_collection = None,\n",
    "                epsilon = 1e-5,\n",
    "                scale = True,\n",
    "                is_training = is_training,\n",
    "                reuse = reuse,\n",
    "                scope = self.name,\n",
    "            )\n",
    "        return self.f(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN:\n",
    "    def __init__(self, img_length, num_colors, d_sizes, g_sizes):\n",
    "        self.img_length = img_length,\n",
    "        self.num_colors = num_colors,\n",
    "        self.latent_dims = g_sizes['z']\n",
    "        \n",
    "        self.X = tf.placeholder(\n",
    "            tf.float32,\n",
    "            shape = (None, self.img_length,self.img_length,self.num_colors),\n",
    "            name = 'X'\n",
    "        )\n",
    "        self.Z = tf.placeholder(\n",
    "            tf.float32,\n",
    "            shape = (None, self.latent_dims),\n",
    "            name = 'Z'\n",
    "        )\n",
    "        self.batch_size = tf.placeholder(\n",
    "            tf.float32,\n",
    "            shape=(),\n",
    "            name = 'batch_size'\n",
    "        )\n",
    "        #build discriminator\n",
    "        logits = self.build_discriminator(self.X, d_sizes)\n",
    "        #build generator\n",
    "        self.sample_images = self.build_generator(self.Z,g_sizes)\n",
    "        \n",
    "        with tf.variable_scope(\"discriminator\") as scope:\n",
    "            scope.reuse_variables()\n",
    "            sample_logits = self.d_forward(self.sample_images, True)\n",
    "        \n",
    "        with tf.variable_scope(\"generator\") as scope:\n",
    "            scope.reuse_variables()\n",
    "            self.sample_image_test = self.g_forward(self.Z, reuse = True, is_training = False)\n",
    "        \n",
    "        #build cost\n",
    "        self.d_cost_real = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits = logits,\n",
    "            labels = tf.ones_like(logits)\n",
    "        )\n",
    "        \n",
    "        self.d_cost_fake = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits = sample_logits,\n",
    "            lables = tf.zeros_like(logits)\n",
    "        )\n",
    "        \n",
    "        self.d_cost = tf.reduce_mean(self.d_cost_real) + tf.reduce_mean(self.d_cost_fake)\n",
    "        \n",
    "        self.g_cost = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                logits = sample_logits,\n",
    "                labels = tf.ones_like(sample_logits)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        real_prediction = tf.cast(logits > 0, tf.float32)\n",
    "        fake_prediction = tf.cast(sample_logits < 0, tf.float32)\n",
    "        num_prediction = 2.0*batch_size\n",
    "        num_correct = tf.reduce_sum(real_prediction) + tf.reduce_sum(fake_prediction)\n",
    "        self.d_accuracy = num_correct/num_prediction\n",
    "        \n",
    "        #optimize\n",
    "        self.d_params = [d for d in tf.trainable_variables() if t.name.startwith('d')]\n",
    "        self.g_params = [g for g in tf.trainable_variables() if t.name.startwith('g')]\n",
    "        \n",
    "        self.d_trainop = tf.train.AdamOptimizer(learning_rate,beta1=beta1).minimize(self.d_cost, var_list = self.d_params)\n",
    "        self.g_trainop = tf.train.AdamOptimizer(learning_rate,beta1=beta1).minimize(self.g_cost, var_list = self.g_params)\n",
    "        \n",
    "        self.initop = tf.global_variables_initializer()\n",
    "        self.s = tf.InteractiveSession()\n",
    "        self.s.run(self.initop)\n",
    "    \n",
    "    def build_discriminator(self, X, d_sizes):\n",
    "        with tf.variable_scope('discriminator') as scope:\n",
    "            \n",
    "            #build conv layer\n",
    "            self.d_convlayers = []\n",
    "            mi = self.num_colors\n",
    "            dim = self.img_length\n",
    "            count = 0\n",
    "            for mo, filter_size, stride, apply_batch_norm in d_sizes['conv_layers']:\n",
    "                name = \"convlayer_%s\"%count\n",
    "                count += 1\n",
    "                \n",
    "                layer = ConvLayer(name, mi, mo,apply_batch_norm, filter_size,stride,lrelu)\n",
    "                self.d_convlayers.append(layer)\n",
    "                mi = mo\n",
    "                dim = int(np.ceil(float(dim)/stride))\n",
    "                \n",
    "            mi = mi*dim*dim\n",
    "            #build dense layer\n",
    "            self.d_denselayers = []\n",
    "            for mo, apply_batch_norm in d_sizes['dense_layers']:\n",
    "                name = \"denselayer_%s\"%count\n",
    "                count += 1\n",
    "                \n",
    "                layer = DenseLayer(name,mi, mo,apply_batch_norm, lrelu)\n",
    "                self.d_denselayers.append(layer)\n",
    "                mi = mo\n",
    "                \n",
    "            #final logistic layer\n",
    "            name = \"denselayer_%s\"%count\n",
    "            \n",
    "            self.d_finallayer = DenseLayer(name,mi,1,False,lambda x:x)\n",
    "            \n",
    "            #get logit\n",
    "            logits = self.d_forward(X)\n",
    "            return logits\n",
    "    \n",
    "    def d_forward(self,X, reuse = None, is_training = True):\n",
    "        output = X\n",
    "        for layer in self.d_convlayers:\n",
    "            output = layer.forward(output,reuse,is_training)\n",
    "        output = tf.contrib.layers.flatten(output)\n",
    "        for layer in self.d_denselayers:\n",
    "            output = layer.forward(output,reuse,is_training)\n",
    "        logits = self.d_finallayer.forward(output,reuse,is_training)\n",
    "        return logits\n",
    "        \n",
    "    \n",
    "    def build_generator(self, Z, g_sizes):\n",
    "        with tf.variable_scope(\"generator\") as scope:\n",
    "            dims = [self.img_length]\n",
    "            dim = self.img_length\n",
    "            for _, _, stride, _ in reversed(g_sizes['conv_layers']):\n",
    "                dim = int(np.ceil(float(dim)/stride))\n",
    "                dims.append(dim)\n",
    "        #dims is backward\n",
    "        dims = list(reversed(dims))\n",
    "        self.g_dims = dims\n",
    "        \n",
    "        #dense layers\n",
    "        mi = self.latent_dims\n",
    "        self.g_denselayers = []\n",
    "        count = 0\n",
    "        for mo, apply_batch_norm in g_sizes['dense_layers']:\n",
    "            name = \"g_denselayer_%s\"%count\n",
    "            count += 1\n",
    "            \n",
    "            layer = DenseLayer(name, mi, mo, apply_batch_norm)\n",
    "            self.g_denselayers.append(layer)\n",
    "            mi = mo\n",
    "        \n",
    "        #final dense layer\n",
    "        mo = g_sizes['projection']*dims[0]*dims[0]\n",
    "        name = \"g_denselayer_%s\"%count\n",
    "        layer = DenseLayer(name,mi,mo,g_sizes['bn_after_projection'])\n",
    "        self.g_denselayers.append(layer)\n",
    "        \n",
    "        #fs-conv layer\n",
    "        mi = g_sizes['projection']\n",
    "        self.g_convlayesr= []\n",
    "        \n",
    "        num_relus = len(g_sizes['conv_layers']) - 1\n",
    "        activation_functions = [tf.nn.relu] * num_relus + [g_sizes['output_activation']]\n",
    "        \n",
    "        for i in range(len(g_sizes['conv_layers'])):\n",
    "            name = \"fs_convlayer_%s\"%i\n",
    "            mo, filter_size,stride, apply_batch_norm = g_sizes['conv_layers'][i]\n",
    "            f = activation_functions[i]\n",
    "            output_shape = [self.batch_size,dims[i+1],dim[i+1],mo]\n",
    "            layer = FractioanllyStridedConvLayer(\n",
    "                name, mi, mo, output_shape, apply_batch_norm, filter_size,stride,f\n",
    "            )\n",
    "            self.g_convlayesr.append(layer)\n",
    "            mi = mo\n",
    "            \n",
    "        self.g_sizes = g_sizes\n",
    "        return self.g_forward(Z)\n",
    "        \n",
    "    \n",
    "    def g_forward(self, Z, reuse = None, is_training = True):\n",
    "        output = Z\n",
    "        for layer in self.g_denselayers:\n",
    "            output = layer.forward(output, reuse, is_training)\n",
    "        \n",
    "        output = tf.reshape(\n",
    "            output,\n",
    "            [-1, self.g_dims[0], self.g_dims[0], self.g_sizes['projection']]\n",
    "        )\n",
    "        \n",
    "        if self.g_sizes['bn_after_projection']:\n",
    "            output = tf.contrib.layers.batch_norm(\n",
    "                output, \n",
    "                decay = 0.9,\n",
    "                updates_collections = None,\n",
    "                epsilon = 1e-5,\n",
    "                scale = True,\n",
    "                is_training = is_training,\n",
    "                reuse = reuse,\n",
    "                scope = 'bn_after_projection',\n",
    "            )\n",
    "            \n",
    "        for layer in self.g_convlayesr:\n",
    "            output = layer.forward(output, reuse, is_training)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    \n",
    "    \n",
    "    def fit(self,X):\n",
    "        d_costs = []\n",
    "        g_costs = []\n",
    "        \n",
    "        N = len(X)\n",
    "        batch_num = int(N//batch_size)\n",
    "        total_iter = 0\n",
    "        \n",
    "        for i in range(epi):\n",
    "            print(\"epi: \"+i)\n",
    "            np.random.shuffle(X)\n",
    "            for j in batch_num:\n",
    "                t0 = datetime.now()\n",
    "                batch = files2image(X[j*batch_size,(j+1)*batch_size])\n",
    "            \n",
    "                Z = np.random.uniform(-1,1,size = (batch_size,latent_dims))\n",
    "                \n",
    "                #train discriminator\n",
    "                _, d_cost, d_acc = self.s.run(\n",
    "                    (self.d_trainop,self.d_cost,self.d_accuracy),\n",
    "                    feed_dict={self.X:X, self.Z:Z, self.batch_size:batch_size},\n",
    "                )\n",
    "                d_costs.append(d_cost)\n",
    "                \n",
    "                #train generator\n",
    "                _, g_cost1 = self.s.run(\n",
    "                    (self.g_trainop,self.g_cost),\n",
    "                    feed_dict = {self.Z:Z,self.batch_size:batch_size},\n",
    "                )\n",
    "                _, g_cost2 = self.s.run(\n",
    "                    (self.g_trainop, self.g_cost),\n",
    "                    feed_dict = {self.Z:Z, self.batch_size:batch_size},\n",
    "                )\n",
    "                g_costs.append((g_cost1+g_cost2)/2)\n",
    "                \n",
    "                print(\"batch%d %d acc%f\"%(i+1,j+1,d_acc))\n",
    "                \n",
    "                plt.clf()\n",
    "                plt.plot(d_costs,label = \"discriminator cost\")\n",
    "                plt.plot(g_costs, label = \"generator cost\")\n",
    "                plt.legend()\n",
    "                plt.savefig(\"cost.png\")\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist(limit = None):\n",
    "    dt = pd.read_csv('C:\\\\Users\\\\Wei Guo\\\\Desktop\\\\data\\\\digit.csv')\n",
    "    dt_value = dt.values\n",
    "    X = dt_value[:,1:]\n",
    "    Y = dt_value[:,0]\n",
    "    if limit != None:\n",
    "        X = X[:int(limit)]\n",
    "        Y = Y[:int(limit)]\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist():\n",
    "    X,Y = get_mnist()\n",
    "    X.reshape(len(X),28,28,1)\n",
    "    dim = X.shape[1]\n",
    "    colors = X.shape[-1]\n",
    "    \n",
    "    #set mnist layers\n",
    "    d_sizes = {\n",
    "        'conv_layers':[(2,5,2,False),(64,5,2,True)],\n",
    "        'dense_layers':[(1024,True)],\n",
    "    }\n",
    "    g_sizes = {\n",
    "        'z':100,\n",
    "        'projection':128,\n",
    "        'bn_after_projection':False,\n",
    "        'conv_layers':[(128,5,2,True),(colors,5,2,False)],\n",
    "        'dense_layers':[(1025,True)],\n",
    "        'output_activation':tf.sigmoid,\n",
    "\n",
    "    }\n",
    "    \n",
    "    gan = DCGAN(dim,colors,d_sizes,g_sizes)\n",
    "    gan.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'tuple'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-1524305cf111>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmnist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-19-4e5c5529c63b>\u001b[0m in \u001b[0;36mmnist\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m     }\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mgan\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDCGAN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0md_sizes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mg_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mgan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-88c9da6b2fef>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, img_length, num_colors, d_sizes, g_sizes)\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimg_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimg_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_colors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m             \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'X'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         )\n\u001b[0;32m     12\u001b[0m         self.Z = tf.placeholder(\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mplaceholder\u001b[1;34m(dtype, shape, name)\u001b[0m\n\u001b[0;32m   1546\u001b[0m     \u001b[0mevaluated\u001b[0m \u001b[0mdirectly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1547\u001b[0m   \"\"\"\n\u001b[1;32m-> 1548\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_placeholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1549\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36m_placeholder\u001b[1;34m(dtype, shape, name)\u001b[0m\n\u001b[0;32m   2092\u001b[0m   \"\"\"\n\u001b[0;32m   2093\u001b[0m   result = _op_def_lib.apply_op(\"Placeholder\", dtype=dtype, shape=shape,\n\u001b[1;32m-> 2094\u001b[1;33m                                 name=name)\n\u001b[0m\u001b[0;32m   2095\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2096\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    703\u001b[0m               [_MakeType(x, attr_def) for x in value])\n\u001b[0;32m    704\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mattr_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"shape\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 705\u001b[1;33m           \u001b[0mattr_value\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_MakeShape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    706\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mattr_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"list(shape)\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    707\u001b[0m           attr_value.list.shape.extend(\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_MakeShape\u001b[1;34m(v, arg_name)\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 198\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_proto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36mas_shape\u001b[1;34m(shape)\u001b[0m\n\u001b[0;32m    831\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    832\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 833\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mTensorShape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    834\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dims)\u001b[0m\n\u001b[0;32m    437\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m         \u001b[1;31m# Got a list of dimensions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mas_dimension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdims_iter\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    440\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    437\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m         \u001b[1;31m# Got a list of dimensions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mas_dimension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdims_iter\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    440\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36mas_dimension\u001b[1;34m(value)\u001b[0m\n\u001b[0;32m    379\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 381\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mDimension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    382\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m     30\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m       if (not isinstance(value, compat.bytes_or_text_types) and\n\u001b[0;32m     34\u001b[0m           self._value != value):\n",
      "\u001b[1;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'tuple'"
     ]
    }
   ],
   "source": [
    "mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
